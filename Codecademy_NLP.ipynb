{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPrvn6z4i4cex4fPrYwTiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elena-Lebedeva/drafts/blob/main/Codecademy_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXT PREPROCESSING\n",
        "Noise Removal"
      ],
      "metadata": {
        "id": "o3NuonxvqwTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .sub() method has three required arguments:\n",
        "\n",
        "pattern – a regular expression that is searched for in the input string. There must be an r preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
        "replacement_text – text that replaces all matches in the input string\n",
        "input – the input string that will be edited by the .sub() method\n",
        "The method returns a string with all instances of the pattern replaced by the replacement_text."
      ],
      "metadata": {
        "id": "3ojRf7GrqP6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "text = \"<p>    This is a paragraph</p>\" \n",
        "result = re.sub(r'<.?p>', '', text)\n",
        "print(result) \n",
        "#    This is a paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20gDSXMzqRxn",
        "outputId": "60ce547b-5ddf-41cf-b1bd-8148a33c699d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    This is a paragraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \\s -  indicates a single whitespace character.\n",
        "# \\w stands for “word character”. It always matches the ASCII characters [A-Za-z0-9_]. \n",
        "# Notice the inclusion of the underscore and digits."
      ],
      "metadata": {
        "id": "DMyXpfJJr-Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"    This is a paragraph\" \n",
        "result = re.sub(r'\\s{4}', '', text)\n",
        "print(result) \n",
        "# This is a paragraph"
      ],
      "metadata": {
        "id": "6ouoPFXWsOzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
        "tweet = '@fat_meats, veggies are better than you think.'\n",
        "headline_no_tag = re.sub(r'</?h1>', '', headline_one)\n",
        "tweet_no_at = re.sub(r'@', '', tweet)"
      ],
      "metadata": {
        "id": "Y_r8-f6Mte3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "text = \"Tokenize this text\"\n",
        "tokenized = word_tokenize(text)\n",
        " \n",
        "print(tokenized)\n",
        "# [\"Tokenize\", \"this\", \"text\"]"
      ],
      "metadata": {
        "id": "0zpvhcApuJ3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To tokenize at the sentence level, we can use sent_tokenize() from the same module.\n",
        "from nltk.tokenize import sent_tokenize\n",
        " \n",
        "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
        "tokenized = sent_tokenize(text)\n",
        " \n",
        "print(tokenized)\n",
        "# ['Tokenize this sentence.', 'Also, tokenize this sentence.']"
      ],
      "metadata": {
        "id": "wiQZ4Po5uVWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
        "\n",
        "tokenized_by_word = word_tokenize(ecg_text)\n",
        "tokenized_by_sentence = sent_tokenize(ecg_text)"
      ],
      "metadata": {
        "id": "kp5nwXMBvGxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "BrfoxmHPvfCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = 'tHiS HaS a MiX oF cAsEs'\n",
        " \n",
        "print(my_string.upper())\n",
        "# 'THIS HAS A MIX OF CASES'\n",
        " \n",
        "print(my_string.lower())\n",
        "# 'this has a mix of cases'"
      ],
      "metadata": {
        "id": "uFOKVZNKvjKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "metadata": {
        "id": "Z8t7Hf5sv-BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english')) \n",
        "nbc_statement = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
        " \n",
        "word_tokens = word_tokenize(nbc_statement) \n",
        "# tokenize nbc_statement\n",
        " \n",
        "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
        " \n",
        "print(statement_no_stop)\n",
        "# ['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']"
      ],
      "metadata": {
        "id": "MWzDqOMlv-0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
        "tokenized_survey = word_tokenize(survey_text)\n",
        "text_no_stops = [word for word in tokenized_survey if word not in stop_words]"
      ],
      "metadata": {
        "id": "55piTHhOyrlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        " is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits."
      ],
      "metadata": {
        "id": "sq2zN4wazGJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
        " \n",
        "stemmed = [stemmer.stem(token) for token in tokenized]\n",
        " \n",
        "print(stemmed)\n",
        "# ['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']"
      ],
      "metadata": {
        "id": "gpXwVvDIzM3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
        "island_tokenized = word_tokenize (populated_island)\n",
        "stemmed = [stemmer.stem(token) for token in island_tokenized]"
      ],
      "metadata": {
        "id": "pYMx-yYu0pmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a method for casting words to their root forms. This is a more involved process than stemming, because it requires the method to know the part of speech for each word. Since lemmatization requires the part of speech, it is a less efficient approach than stemming."
      ],
      "metadata": {
        "id": "LVTDhpFx07Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
        " \n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
        " \n",
        "print(lemmatized)\n",
        "# [\"NBC\", \"wa\", \"founded\", \"in\", \"1926\"]"
      ],
      "metadata": {
        "id": "dwUGhLQ507-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
        "\n",
        "tokenized_string = word_tokenize(populated_island)\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]"
      ],
      "metadata": {
        "id": "brKr_8yE15Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-Speech Tagging\n",
        "To improve the performance of lemmatization, we need to find the part of speech for each word in our string. In script.py, to the right, we created a part-of-speech tagging function. The function accepts a word, then returns the most common part of speech for that word."
      ],
      "metadata": {
        "id": "-UVNX0b02xWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "# Inside of our function, we use the wordnet.synsets() function to get a set of synonyms for the word:\n",
        "# The returned synonyms come with their part of speech.\n",
        "def get_part_of_speech(word):\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "\n",
        "#Next, we create a Counter() object and set each value to the count of the number of synonyms that fall into each part of speech:\n",
        "pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "... \n",
        "# This line counts the number of nouns in the synonym set.\n",
        "\n",
        "# Now that we have a count for each part of speech, \n",
        "# we can use the .most_common() counter method to find and return the most likely part of speech:\n",
        "most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "\n",
        "# Now that we can find the most probable part of speech for a given word, \n",
        "# we can pass this into our lemmatizer when we find the root for each word.\n",
        "tokenized = [\"How\", \"old\", \"is\", \"the\", \"country\", \"Indonesia\"]\n",
        " \n",
        "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
        " \n",
        "print(lemmatized)\n",
        "# ['How', 'old', 'be', 'the', 'country', 'Indonesia']\n",
        "# Previously: ['How', 'old', 'is', 'the', 'country', 'Indonesia']"
      ],
      "metadata": {
        "id": "u2J9ERvL271X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "\n",
        "def get_part_of_speech(word):\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "  \n",
        "  pos_counts = Counter()\n",
        "\n",
        "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
        "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
        "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
        "  \n",
        "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "  return most_likely_part_of_speech"
      ],
      "metadata": {
        "id": "0LRd4OuB5zF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from part_of_speech import get_part_of_speech\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
        "\n",
        "tokenized_string = word_tokenize(populated_island)\n",
        "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]\n",
        "\n",
        "\n",
        "try:\n",
        "  print(f'The lemmatized words are: {lemmatized_pos}')\n",
        "except:\n",
        "  print('Expected a variable called `lemmatized_pos`')"
      ],
      "metadata": {
        "id": "o_dAVhCC5zVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARSING WITH REGULAR EXPRESSIONS"
      ],
      "metadata": {
        "id": "8E4WjB5X50Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling and Matching\n",
        ".compile(). This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. The regular expression object below will exactly match 4 upper or lower case characters.\n",
        "\n",
        "Regular expression objects have a .match() method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string.\n",
        "\n",
        "If .match() finds a match that starts at the beginning of the string, it will return a match object. The match object lets you know what piece of text the regular expression matched, and at what index the match begins and ends. If there is no match, .match() will return None.\n",
        "\n",
        "With the match object stored in result, you can access the matched text by calling result.group(0). If you use a regex containing capture groups, you can access these groups by calling .group() with the appropriately numbered capture group as an argument.\n",
        "\n",
        "Instead of compiling the regular expression first and then looking for a match in separate lines of code, you can simplify your match to one line:"
      ],
      "metadata": {
        "id": "UCBLnlp16iz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regular_expression_object = re.compile(\"[A-Za-z]{4}\")\n",
        "result = regular_expression_object.match(\"Toto\")\n",
        "result = re.match(\"[A-Za-z]{4}\",\"Toto\")\n",
        "# With this syntax, re‘s .match() method takes a regular expression pattern as the first argument and a string as the second argument"
      ],
      "metadata": {
        "id": "ADjiktkF535i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# characters are defined\n",
        "character_1 = \"Dorothy\"\n",
        "character_2 = \"Henry\"\n",
        "\n",
        "# compile your regular expression here\n",
        "regular_expression = re.compile(\"[A-Za-z]{7}\")\n",
        "\n",
        "# check for a match to character_1 here\n",
        "result_1 = regular_expression.match(character_1)\n",
        "print(result_1)\n",
        "\n",
        "# store and print the matched text here\n",
        "match_1 = result_1.group(0)\n",
        "print(match_1)\n",
        "\n",
        "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
        "result_2 = re.match(\"[A-Za-z]{7}\",character_2)\n",
        "print(result_2)"
      ],
      "metadata": {
        "id": "i-vOTBtc-sbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".search() method. Unlike .match() which will only find matches at the start of a string, .search() will look left to right through an entire piece of text and return a match object for the first match to the regular expression given. If no match is found, .search() will return None.\n",
        "Given a regular expression as its first argument and a string as its second argument, .findall() will return a list of all non-overlapping matches of the regular expression in the string. "
      ],
      "metadata": {
        "id": "fgwin5KW_ss_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.search(\"\\w{8}\",\"Are you a Munchkin?\")"
      ],
      "metadata": {
        "id": "6Go6yRMM-tXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
        "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
        "\n",
        "# search oz_text for an occurrence of 'wizard' here\n",
        "found_wizard = re.search(\"wizard\",oz_text)\n",
        "print(found_wizard)\n",
        "all_lions = re.findall(\"lion\", oz_text) \n",
        "print(all_lions)\n",
        "number_lions = len(all_lions)\n",
        "print(number_lions)"
      ],
      "metadata": {
        "id": "m6GeDhLSBUqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP)\n",
        "Pronoun: a word used in place of a noun (her,she)\n",
        "Determiner: a word that introduces, or “determines”, a noun (the)\n",
        "Verb: expresses action (studying) or being (are,has)\n",
        "Adjective: modifies or describes a noun or pronoun (new)\n",
        "Adverb: modifies or describes a verb, an adjective, or another adverb (happily)\n",
        "Preposition: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on)\n",
        "Conjunction: a word that joins words, phrases, or clauses (and)\n",
        "Interjection: a word used to express emotion (Wow).\n",
        "\n",
        "You can automate the part-of-speech tagging process with nltk‘s pos_tag() function! The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag."
      ],
      "metadata": {
        "id": "mOiMHWDtCPIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from word_tokenized_oz import word_tokenized_oz\n",
        "\n",
        "# save and print the sentence stored at index 100 in word_tokenized_oz here\n",
        "witches_fate = word_tokenized_oz[100]\n",
        "print(witches_fate)\n",
        "\n",
        "# create a list to hold part-of-speech tagged sentences here\n",
        "pos_tagged_oz = []\n",
        "\n",
        "# create a for loop through each word tokenized sentence in word_tokenized_oz here\n",
        "for word_tokenized_sentence in word_tokenized_oz:\n",
        "    part_of_speech = pos_tag(word_tokenized_sentence)\n",
        "    pos_tagged_oz.append(part_of_speech)\n",
        "\n",
        "  # part-of-speech tag each sentence and append to pos_tagged_oz here\n",
        "  \n",
        "\n",
        "# store and print the 101st part-of-speech tagged sentence here\n",
        "witches_fate_pos = pos_tagged_oz[100]\n",
        "print(witches_fate_pos)"
      ],
      "metadata": {
        "id": "qehutKHGCPxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique of grouping words by their part-of-speech tag is called chunking.\n",
        "With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions. You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text.\n",
        "\n",
        "The regular expression you build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:"
      ],
      "metadata": {
        "id": "UxiPXiimlGgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_grammar = \"AN: {<JJ><NN>}\""
      ],
      "metadata": {
        "id": "V4R1eqMEmHT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AN is a user-defined name for the kind of chunk you are searching for. You can use whatever name makes sense given your chunk grammar. In this case AN stands for adjective-noun.\n",
        "\n",
        "A pair of curly braces {} surround the actual chunk grammar\n",
        "JJ operates similarly to a regex character class, matching any adjective\n",
        "NN matches any noun, singular or plural.\n",
        "\n",
        "The chunk grammar above will thus match any adjective that is followed by a noun.\n",
        "\n",
        "To use the chunk grammar defined, you must create a nltk RegexpParser object and give it a piece of chunk grammar as an argument."
      ],
      "metadata": {
        "id": "Jprfns6xmSzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = RegexpParser(chunk_grammar)"
      ],
      "metadata": {
        "id": "nuxiR8FLmM2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then use the RegexpParser object’s .parse() method, which takes a list of part-of-speech tagged words as an argument, and identifies where such chunks occur in the sentence!\n",
        "\n",
        "Consider the part-of-speech tagged sentence below:"
      ],
      "metadata": {
        "id": "tzkqD4LtmwOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_sentence = [('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.')]"
      ],
      "metadata": {
        "id": "FCgRHOmRm2Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked = chunk_parser.parse(pos_tagged_sentence)"
      ],
      "metadata": {
        "id": "38ZT55bTm37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser, Tree\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "\n",
        "# define adjective-noun chunk grammar here\n",
        "chunk_grammar = \"AN: {<JJ><NN>}\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# chunk the pos-tagged sentence at index 282 in pos_tagged_oz here\n",
        "\n",
        "scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])\n",
        "print(scaredy_cat)\n",
        "# pretty_print the chunked sentence here\n",
        "Tree.fromstring(str(scaredy_cat)).pretty_print()\n",
        "print(pos_tagged_oz[282])"
      ],
      "metadata": {
        "id": "_9ay3c4op_6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking Noun Phrases or NP-chunking"
      ],
      "metadata": {
        "id": "ECtmS7RHqGSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "kKJYG4CVqBv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase\n",
        "DT> matches any determiner\n",
        "? is an optional quantifier, matching either 0 or 1 determiners\n",
        "JJ> matches any adjective\n",
        "\"*\" is the Kleene star quantifier, matching 0 or more occurrences of an adjective\n",
        "NN> matches any noun, singular or plural"
      ],
      "metadata": {
        "id": "m_HqDKnXrMt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "from np_chunk_counter import np_chunk_counter\n",
        "\n",
        "# define noun-phrase chunk grammar here\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# create a list to hold noun-phrase chunked sentences\n",
        "np_chunked_oz = list()\n",
        "\n",
        "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
        "np_chunked_oz = []\n",
        "for sentence in pos_tagged_oz:\n",
        "  result = chunk_parser.parse(sentence)\n",
        "  # chunk each sentence and append to np_chunked_oz here\n",
        "  np_chunked_oz.append(result)\n",
        "\n",
        "# store and print the most common np-chunks here\n",
        "# NOTE: np_chunk_counter function was already built\n",
        "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
        "print(most_common_np_chunks)"
      ],
      "metadata": {
        "id": "4EGLZijwtlgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking Verb Phrases\n",
        "The first structure begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb."
      ],
      "metadata": {
        "id": "KIsbu7OG_WkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\""
      ],
      "metadata": {
        "id": "vz13QoLHtm6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VP is the user-defined name of the chunk you are searching for. In this case VP stands for verb phrase\n",
        "VB.*> matches any verb using the . as a wildcard and the * quantifier to match 0 or more occurrences of any character. This ensures matching verbs of any tense (ex. VB for present tense, VBD for past tense, or VBN for past participle)\n",
        "DT>?JJ>*NN> matches any noun phrase\n",
        "RB.?> matches any adverb using the . as a wildcard and the optional quantifier to match 0 or 1 occurrence of any character. This ensures matching any form of adverb (regular RB, comparative RBR, or superlative RBS)\n",
        "? is an optional quantifier, matching either 0 or 1 adverbs"
      ],
      "metadata": {
        "id": "zUPcLjUYAI7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
        "# The chunk grammar for the second form of verb phrase is given below:"
      ],
      "metadata": {
        "id": "hBrmnEgEAQF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "from vp_chunk_counter import vp_chunk_counter\n",
        "\n",
        "# define verb phrase chunk grammar here\n",
        "chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# create a list to hold verb-phrase chunked sentences\n",
        "vp_chunked_oz = list()\n",
        "\n",
        "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
        "for sentence in pos_tagged_oz:\n",
        "  result = chunk_parser.parse(sentence)\n",
        "  # chunk each sentence and append to vp_chunked_oz here\n",
        "  vp_chunked_oz.append(result)\n",
        "  \n",
        "# store and print the most common vp-chunks here\n",
        "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
        "print(most_common_vp_chunks)"
      ],
      "metadata": {
        "id": "jgIp1hIvAqVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunk Filtering\n",
        "Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them.\n",
        "\n",
        "A popular method for performing chunk filtering is to chunk an entire sentence together and then indicate which parts of speech are to be filtered out. If the filtered parts of speech are in the middle of a chunk, it will split the chunk into two separate chunks! "
      ],
      "metadata": {
        "id": "CfvrB3e7hrW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_grammar = \"\"\"NP: {<.*>+}\n",
        "                       }<VB.?|IN>+{\"\"\""
      ],
      "metadata": {
        "id": "jvQ7nlbykTme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase\n",
        "The brackets {} indicate what parts of speech you are chunking. .*>+ matches every part of speech in the sentence\n",
        "The inverted brackets }{ indicate which parts of speech you want to filter from the chunk. VB.?|IN>+ will filter out any verbs or prepositions"
      ],
      "metadata": {
        "id": "MtN2KRAzkiRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser, Tree\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "\n",
        "# define chunk grammar to chunk an entire sentence together\n",
        "grammar = \"Chunk: {<.*>+}\"\n",
        "\n",
        "# create RegexpParser object\n",
        "parser = RegexpParser(grammar)\n",
        "\n",
        "# chunk the pos-tagged sentence at index 230 in pos_tagged_oz\n",
        "chunked_dancers = parser.parse(pos_tagged_oz[230])\n",
        "print(chunked_dancers)\n",
        "\n",
        "# define noun phrase chunk grammar using chunk filtering here\n",
        "chunk_grammar = \"\"\"NP: {<.*>+}\n",
        "                       }<VB.?|IN>+{\"\"\"\n",
        "\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here\n",
        "filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])\n",
        "print(filtered_dancers)\n",
        "\n",
        "# pretty_print the chunked and filtered sentence here\n",
        "Tree.fromstring(str(filtered_dancers)).pretty_print()"
      ],
      "metadata": {
        "id": "fq7pkw_3ogPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Chunk\n",
        "    then/RB\n",
        "    she/PRP\n",
        "    sat/VBD\n",
        "    upon/IN\n",
        "    a/DT\n",
        "    settee/NN\n",
        "    and/CC\n",
        "    watched/VBD\n",
        "    the/DT\n",
        "    people/NNS\n",
        "    dance/NN"
      ],
      "metadata": {
        "id": "0wubv77QpQTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instead of finding NP-chunks or VP-chunks, define your own chunk grammar using regular expressions in between the curly braces {}. \n",
        "# Feel free to add any chunk filtering in between the inverted braces }{ if you so desire!\n",
        "\n",
        "from nltk import RegexpParser\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "from chunk_counter import chunk_counter\n",
        "\n",
        "# define your own chunk grammar here\n",
        "chunk_grammar = '''Chunk: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t}<DT.?|IN|RB|CC>+{'''\n",
        "\n",
        "# create RegexpParser object\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# create a list to hold chunked sentences\n",
        "chunked_oz = list()\n",
        "\n",
        "# create a for loop through each pos-tagged sentence in pos_tagged_oz\n",
        "for pos_tagged_sentence in pos_tagged_oz:\n",
        "  # chunk each sentence and append to chunked_oz\n",
        "  chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
        "\n",
        "# store and print the most common chunks\n",
        "most_common_chunks = chunk_counter(chunked_oz)\n",
        "print(most_common_chunks)"
      ],
      "metadata": {
        "id": "aFUCWUqfqU7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words language model"
      ],
      "metadata": {
        "id": "3gwwjZ4ZBnbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from spam_data import training_spam_docs, training_doc_tokens, training_labels\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from preprocessing import preprocess_text\n",
        "\n",
        "# Add your email text to test_text between the triple quotes:\n",
        "test_text = \"\"\"\n",
        "Ja és tradició començar l’any amb el bon propòsit de la música. Et convidem a compondre el teu 2023 amb una selecció de grans compositors que protagonitzen la programació dels propers mesos. Energies renovades per endinsar-te en els genis, que no n’entenen, del pas del temps, i també en els compositors convidats, que obren nous camins.\n",
        "\"\"\"\n",
        "test_tokens = preprocess_text(test_text)\n",
        "\n",
        "def create_features_dictionary(document_tokens):\n",
        "  features_dictionary = {}\n",
        "  index = 0\n",
        "  for token in document_tokens:\n",
        "    if token not in features_dictionary:\n",
        "      features_dictionary[token] = index\n",
        "      index += 1\n",
        "  return features_dictionary\n",
        "\n",
        "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
        "  bow_vector = [0] * len(features_dictionary)\n",
        "  for token in document_tokens:\n",
        "    if token in features_dictionary:\n",
        "      feature_index = features_dictionary[token]\n",
        "      bow_vector[feature_index] += 1\n",
        "  return bow_vector\n",
        "\n",
        "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
        "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
        "test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]\n",
        "\n",
        "spam_classifier = MultinomialNB()\n",
        "spam_classifier.fit(training_vectors, training_labels)\n",
        "\n",
        "predictions = spam_classifier.predict(test_vectors)\n",
        "\n",
        "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
      ],
      "metadata": {
        "id": "VRpxJsAoBvUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discover Insights into Classic Texts"
      ],
      "metadata": {
        "id": "x68FgcwoohkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, RegexpParser\n",
        "from tokenize_words import word_sentence_tokenize\n",
        "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
        "\n",
        "# import text of choice here\n",
        "text = open(\"dorian_gray.txt\", encoding = 'utf-8').read().lower()\n",
        "\n",
        "# sentence and word tokenize text here\n",
        "word_tokenized_text = word_sentence_tokenize(text)\n",
        "\n",
        "# store and print any word tokenized sentence here\n",
        "single_word_tokenized_sentence = word_tokenized_text[1]\n",
        "# print(single_word_tokenized_sentence)\n",
        "\n",
        "# create a list to hold part-of-speech tagged sentences here\n",
        "pos_tagged_text = []\n",
        "\n",
        "# create a for loop through each word tokenized sentence here\n",
        "for sentence in word_tokenized_text:\n",
        "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
        "  pos_tagged_text.append(pos_tag(sentence))\n",
        "\n",
        "# store and print any part-of-speech tagged sentence here\n",
        "single_pos_sentence = pos_tagged_text[1]\n",
        "# print(single_pos_sentence)\n",
        "\n",
        "# define noun phrase chunk grammar here\n",
        "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "# create noun phrase RegexpParser object here\n",
        "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
        "\n",
        "# define verb phrase chunk grammar here\n",
        "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
        "\n",
        "# create verb phrase RegexpParser object here\n",
        "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
        "\n",
        "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
        "np_chunked_text = []\n",
        "vp_chunked_text = []\n",
        "\n",
        "# create a for loop through each pos-tagged sentence here\n",
        "for sentence in pos_tagged_text:\n",
        "  # chunk each sentence and append to lists here\n",
        "  np_chunked_text.append(np_chunk_parser.parse(sentence))\n",
        "  vp_chunked_text.append(vp_chunk_parser.parse(sentence))\n",
        "  \n",
        "\n",
        "# store and print the most common NP-chunks here\n",
        "most_common_np_chunk = np_chunk_counter(np_chunked_text)\n",
        "print(most_common_np_chunk)\n",
        "print(\"***\")\n",
        "# store and print the most common VP-chunks here\n",
        "most_common_vp_chunk = vp_chunk_counter(vp_chunked_text)\n",
        "print(most_common_vp_chunk)"
      ],
      "metadata": {
        "id": "jf1njr0torkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of the words"
      ],
      "metadata": {
        "id": "alWPYL91gZE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing function"
      ],
      "metadata": {
        "id": "KPGUM5zm4Gd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "import nltk, re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "normalizer = WordNetLemmatizer()\n",
        "\n",
        "def get_part_of_speech(word):\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "  pos_counts = Counter()\n",
        "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
        "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
        "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
        "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "  return most_likely_part_of_speech\n",
        "\n",
        "def preprocess_text(text):\n",
        "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
        "  tokenized = word_tokenize(cleaned)\n",
        "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
        "  return normalized"
      ],
      "metadata": {
        "id": "oObjgm84gxJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import preprocess_text\n",
        "# Define text_to_bow() below:\n",
        "def text_to_bow(some_text):\n",
        "  bow_dictionary = {}\n",
        "  tokens = preprocess_text(some_text)\n",
        "  for token in tokens:\n",
        "    if token in bow_dictionary:\n",
        "      bow_dictionary[token] += 1\n",
        "    else:\n",
        "      bow_dictionary[token] = 1\n",
        "  return bow_dictionary\n",
        "\n",
        "\n",
        "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))"
      ],
      "metadata": {
        "id": "xID4BNxmgcFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectors and Building a Features Dictionary"
      ],
      "metadata": {
        "id": "BokcbJpmL3rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a Features Dictionary\n",
        "from preprocessing import preprocess_text\n",
        "# Define create_features_dictionary() below:\n",
        "\n",
        "\n",
        "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
        "\n",
        "def create_features_dictionary(documents):\n",
        "  features_dictionary = {}\n",
        "  merged = \" \".join(documents)\n",
        "  tokens = preprocess_text(merged)\n",
        "  index = 0\n",
        "  for token in tokens:\n",
        "    if token not in features_dictionary:\n",
        "      features_dictionary[token] = index\n",
        "      index += 1\n",
        "  return features_dictionary,tokens\n",
        "\n",
        "print(create_features_dictionary(training_documents)[0])"
      ],
      "metadata": {
        "id": "G1wvMoWMR2KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output is the dictionary with all the words and their respective indexes\n",
        "# {'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}"
      ],
      "metadata": {
        "id": "ED2rqu5sR27F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a BoW Vector\n",
        "from preprocessing import preprocess_text\n",
        "# Define text_to_bow_vector() below:\n",
        "def text_to_bow_vector(some_text, features_dictionary):\n",
        "  bow_vector = len(features_dictionary)*[0]\n",
        "  tokens = preprocess_text(some_text)\n",
        "  for token in tokens:\n",
        "    feature_index = features_dictionary[token]\n",
        "    bow_vector[feature_index] += 1\n",
        "  return bow_vector, tokens\n",
        "\n",
        "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
        "\n",
        "text = \"Another five fish find another faraway fish.\"\n",
        "\n",
        "print(text_to_bow_vector(text, features_dictionary)[0])"
      ],
      "metadata": {
        "id": "g7vh_Ie9SHRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "def create_features_dictionary(document_tokens):\n",
        "  features_dictionary = {}\n",
        "  index = 0\n",
        "  for token in document_tokens:\n",
        "    if token not in features_dictionary:\n",
        "      features_dictionary[token] = index\n",
        "      index += 1\n",
        "  return features_dictionary\n",
        "\n",
        "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
        "  bow_vector = [0] * len(features_dictionary)\n",
        "  for token in document_tokens:\n",
        "    if token in features_dictionary:\n",
        "      feature_index = features_dictionary[token]\n",
        "      bow_vector[feature_index] += 1\n",
        "  return bow_vector\n",
        "\n",
        "# Define bow_sms_dictionary:\n",
        "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
        "\n",
        "# Define training_vectors:\n",
        "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
        "# Define test_vectors:\n",
        "test_vectors = [tokens_to_bow_vector(test_doc, bow_sms_dictionary) for test_doc in test_spam_docs]\n",
        "\n",
        "spam_classifier = MultinomialNB()\n",
        "\n",
        "def spam_or_not(label):\n",
        "  return \"spam\" if label else \"not spam\"\n",
        "\n",
        "# Uncomment the code below when you're done:\n",
        "spam_classifier.fit(training_vectors, training_labels)\n",
        "\n",
        "predictions = spam_classifier.score(test_vectors, test_labels)\n",
        "\n",
        "'''print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' \n",
        "was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100,\n",
        "test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))\n",
        "'''"
      ],
      "metadata": {
        "id": "g1V3MILcYfo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For text_to_bow(), you can approximate the functionality with the collections module’s Counter() function:"
      ],
      "metadata": {
        "id": "iEThmn3GZFG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        " \n",
        "tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
        "print(Counter(tokens))\n",
        " \n",
        "# Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})"
      ],
      "metadata": {
        "id": "dpuP9qKVY_5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:"
      ],
      "metadata": {
        "id": "TVPpWtkcZGBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
        "test_text = [\"Another five fish find another faraway fish.\"]\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_vectorizer.fit(training_documents)\n",
        "bow_vector = bow_vectorizer.transform(test_text)\n",
        "print(bow_vector.toarray())\n",
        "# [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]"
      ],
      "metadata": {
        "id": "lXWqEL8iZH3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import CountVectorizer from sklearn:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define bow_vectorizer:\n",
        "bow_vectorizer = CountVectorizer()\n",
        "\n",
        "# Define training_vectors:\n",
        "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
        "# Define test_vectors:\n",
        "test_vectors = bow_vectorizer.transform(test_docs)\n",
        "\n",
        "spam_classifier = MultinomialNB()\n",
        "\n",
        "def spam_or_not(label):\n",
        "  return \"spam\" if label else \"not spam\"\n",
        "\n",
        "# Uncomment the code below when you're done:\n",
        "spam_classifier.fit(training_vectors, training_labels)\n",
        "\n",
        "predictions = spam_classifier.score(test_vectors, test_labels)\n",
        "\n",
        "'''print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, \n",
        "'{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(\n",
        "  predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[15], spam_or_not(test_labels[15])))\n",
        "'''"
      ],
      "metadata": {
        "id": "zyZoIuS7avPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity (i.e., it has more training knowledge to draw from) than other statistical models.\n",
        "\n",
        "Overfitting (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.\n",
        "\n",
        "The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models."
      ],
      "metadata": {
        "id": "O4Smg6QUdfMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import preprocess_text\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "text = \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
        "tokens = preprocess_text(text)\n",
        "\n",
        "# Bigram approach:\n",
        "bigrams_prepped = ngrams(tokens, 2)\n",
        "bigrams = Counter(bigrams_prepped)\n",
        "print(\"Three most frequent word sequences and the number of occurrences according to Bigrams:\")\n",
        "print(bigrams.most_common(3))\n",
        "\n",
        "# Bag-of-Words approach:\n",
        "# Define bag_of_words here:\n",
        "bag_of_words = Counter(tokens)\n",
        "most_common_three = bag_of_words.most_common(3)\n",
        "print(\"\\nThree most frequent words and number of occurrences according to Bag-of-Words:\")\n",
        "print(most_common_three)"
      ],
      "metadata": {
        "id": "POa3D96bdkuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because bag-of-words has high perplexity, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.\n",
        " \n",
        "Like all statistical models, BoW suffers from overfitting when it comes to vocabulary.\n",
        "\n",
        "There are several ways that NLP developers have tackled this issue. A common approach is through language smoothing in which some probability is siphoned from the known words and given to unknown words."
      ],
      "metadata": {
        "id": "0uQenQYle4bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, deque, Counter\n",
        "from document import oscar_wilde_thoughts\n",
        "\n",
        "# Change sequence_length:\n",
        "sequence_length = 1\n",
        "\n",
        "class MarkovChain:\n",
        "  def __init__(self):\n",
        "    self.lookup_dict = defaultdict(list)\n",
        "    self.most_common = []\n",
        "    self._seeded = False\n",
        "    self.__seed_me()\n",
        "\n",
        "  def __seed_me(self, rand_seed=None):\n",
        "    if self._seeded is not True:\n",
        "      try:\n",
        "        if rand_seed is not None:\n",
        "          random.seed(rand_seed)\n",
        "        else:\n",
        "          random.seed()\n",
        "        self._seeded = True\n",
        "      except NotImplementedError:\n",
        "        self._seeded = False\n",
        "    \n",
        "  def add_document(self, str):\n",
        "    preprocessed_list = self._preprocess(str)\n",
        "    self.most_common = Counter(preprocessed_list).most_common(50)\n",
        "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
        "    for pair in pairs:\n",
        "      self.lookup_dict[pair[0]].append(pair[1])\n",
        "  \n",
        "  def _preprocess(self, str):\n",
        "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
        "    tokenized = word_tokenize(cleaned)\n",
        "    return tokenized\n",
        "\n",
        "  def __generate_tuple_keys(self, data):\n",
        "    if len(data) < sequence_length:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 1):\n",
        "      yield [ data[i], data[i + 1] ]\n",
        "      \n",
        "  def generate_text(self, max_length=50):\n",
        "    context = deque()\n",
        "    output = []\n",
        "    if len(self.lookup_dict) > 0:\n",
        "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
        "      chain_head = [list(self.lookup_dict)[0]]\n",
        "      context.extend(chain_head)\n",
        "      if sequence_length > 1:\n",
        "        while len(output) < (max_length - 1):\n",
        "          next_choices = self.lookup_dict[context[-1]]\n",
        "          if len(next_choices) > 0:\n",
        "            next_word = random.choice(next_choices)\n",
        "            context.append(next_word)\n",
        "            output.append(context.popleft())\n",
        "          else:\n",
        "            break\n",
        "        output.extend(list(context))\n",
        "      else:\n",
        "        while len(output) < (max_length - 1):\n",
        "          next_choices = [word[0] for word in self.most_common]\n",
        "          next_word = random.choice(next_choices)\n",
        "          output.append(next_word)\n",
        "    return \" \".join(output)\n",
        "\n",
        "my_markov = MarkovChain()\n",
        "my_markov.add_document(oscar_wilde_thoughts)\n",
        "random_oscar_wilde = my_markov.generate_text()\n",
        "print(random_oscar_wilde)"
      ],
      "metadata": {
        "id": "wq-2R_TGfPcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review of Bag-of-Words\n",
        "You made it! And you’ve learned plenty about the bag-of-words language model along the way:\n",
        "\n",
        "Bag-of-words (BoW) — also referred to as the unigram model — is a statistical language model based on word count.\n",
        "There are loads of real-world applications for BoW.\n",
        "BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.\n",
        "For BoW, training data is the text that is used to build a BoW model.\n",
        "BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.\n",
        "A feature vector is a numeric depiction of an item’s salient features.\n",
        "Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
        "A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
        "BoW has less data sparsity than other statistical models. It also suffers less from overfitting.\n",
        "BoW has higher perplexity than other models, making it less ideal for language prediction.\n",
        "One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words."
      ],
      "metadata": {
        "id": "uAW5HtjxgkBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY NATURAL LANGUAGE PROCESSING WITH PYTHON\n",
        "# Mystery Friend\n",
        "\n",
        "from goldman_emma_raw import goldman_docs\n",
        "from henson_matthew_raw import henson_docs\n",
        "from wu_tingfang_raw import wu_docs\n",
        "# import sklearn modules here:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Setting up the combined list of friends' writing samples\n",
        "friends_docs = goldman_docs + henson_docs + wu_docs\n",
        "# Setting up labels for your three friends\n",
        "friends_labels = [1] *len(goldman_docs) + [2] *len(henson_docs) + [3] *len(wu_docs)\n",
        "# Print out a document from each friend:\n",
        "\n",
        "\n",
        "mystery_postcard = \"\"\"\n",
        "I do not pretend to write a history. Removed by fifty or a hundred years from the events he is describing, the historian may seem to be objective. But real history is not a compilation of mere data. It is valueless without the human[Pg vii] element which the historian necessarily gets from the writings of the contemporaries of the events in question. It is the personal reactions of the participants and observers which lend vitality to all history and make it vivid and alive. Thus, numerous histories have been written of the French Revolution; yet there are only a very few that stand out true and convincing, illuminative in the degree in which the historian has felt his subject through the medium of human documents left by the contemporaries of the period.\n",
        "\"\"\"\n",
        "\n",
        "# Create bow_vectorizer:\n",
        "bow_vectorizer = CountVectorizer()\n",
        "\n",
        "# Define friends_vectors:\n",
        "friends_vectors = bow_vectorizer.fit_transform(friends_docs)\n",
        "\n",
        "# Define mystery_vector: \n",
        "# mystery_postcard is a string, while the vectorizer expects a list as an argument\n",
        "mystery_vector =  bow_vectorizer.transform([mystery_postcard])\n",
        "# Define friends_classifier:\n",
        "friends_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier:\n",
        "friends_classifier.fit(friends_vectors, friends_labels)\n",
        "\n",
        "# Change predictions:\n",
        "#predictions = friends_classifier.predict(mystery_vector)\n",
        "predictions = friends_classifier.predict(mystery_vector)\n",
        "mystery_friend = predictions[0] if predictions[0] else \"someone else\"\n",
        "\n",
        "# Uncomment the print statement:\n",
        "print(\"The postcard was from {}!\".format(mystery_friend))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "F66We2qNgk-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does mystery_bow_function() do?\n",
        "# The dictionary is comprised of words with their corresponding counts.\n",
        "\n",
        "def mystery_bow_function(training_data):\n",
        "  bow_dictionary = {}\n",
        "  tokens = preprocess_text(training_data)\n",
        "  for token in tokens:\n",
        "    if token in bow_dictionary:\n",
        "      bow_dictionary[token] += 1\n",
        "    else:\n",
        "      bow_dictionary[token] = 1\n",
        "  return bow_dictionary\n",
        " \n",
        "print(mystery_bow_function(\"Squealing suitcase squids are not like regular squids.\"))\n",
        " \n",
        "# {'regular': 1, 'squeal': 1, 'squid': 2, 'be': 1, 'like': 1, 'suitcase': 1, 'not': 1}"
      ],
      "metadata": {
        "id": "Sy2VbZMdAyID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TERM FREQUENCY–INVERSE DOCUMENT FREQUENCY"
      ],
      "metadata": {
        "id": "YNrkSDgWz7nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tf-idf is another powerful tool in your NLP toolkit that has a variety of use cases included:\n",
        "\n",
        "ranking results in a search engine\n",
        "text summarization\n",
        "building smarter chatbots"
      ],
      "metadata": {
        "id": "hHer5jgt0Spv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term frequency-inverse document frequency is a numerical statistic used to indicate how important a word is to each document in a collection of documents, or a corpus."
      ],
      "metadata": {
        "id": "lulwvHIi0xvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tf-idf relies on two different metrics in order to come up with an overall score:\n",
        "\n",
        "term frequency, or how often a word appears in a document. This is the same as bag-of-words’ word count.\n",
        "inverse document frequency, which is a measure of how often a word appears in the overall corpus. By penalizing the score of words that appear throughout a corpus, tf-idf can give better insight into how important a word is to a particular document of a corpus.\n",
        "We will dig into each component of tf-idf in the next two exercises."
      ],
      "metadata": {
        "id": "x_jKAUzM1K-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "from preprocessing import preprocess_text\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# sample documents\n",
        "document_1 = \"This is a sample sentence!\"\n",
        "document_2 = \"This is my second sentence.\"\n",
        "document_3 = \"Is this my third sentence?\"\n",
        "\n",
        "# corpus of documents\n",
        "corpus = [document_1, document_2, document_3]\n",
        "\n",
        "# preprocess documents\n",
        "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# initialize and fit TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "corpus_index = [n for n in processed_corpus]\n",
        "\n",
        "# create pandas DataFrame with tf-idf scores\n",
        "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
        "print(df_tf_idf)"
      ],
      "metadata": {
        "id": "6SiKmx8Jz8kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from preprocessing import preprocess_text\n",
        "\n",
        "poem = '''\n",
        "Success is counted sweetest\n",
        "By those who ne'er succeed.\n",
        "To comprehend a nectar\n",
        "Requires sorest need.\n",
        "\n",
        "Not one of all the purple host\n",
        "Who took the flag to-day\n",
        "Can tell the definition,\n",
        "So clear, of victory,\n",
        "\n",
        "As he, defeated, dying,\n",
        "On whose forbidden ear\n",
        "The distant strains of triumph\n",
        "Break, agonized and clear!'''\n",
        "\n",
        "# define clear_count:\n",
        "clear_count = 2\n",
        "\n",
        "# preprocess text\n",
        "processed_poem = preprocess_text(poem)\n",
        "\n",
        "# initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "term_frequencies = vectorizer.fit_transform([processed_poem])\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# create pandas DataFrame with term frequencies\n",
        "try:\n",
        "  df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=['Term Frequency'])\n",
        "  print(df_term_frequencies)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "svmv1OUe2Kuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse document frequency can be calculated on a group of documents using scikit-learn’s TfidfTransformer:"
      ],
      "metadata": {
        "id": "v4tE3dMV43Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TfidfTransformer(norm=None)\n",
        "transformer.fit(term_frequencies)\n",
        "inverse_doc_frequency = transformer.idf_"
      ],
      "metadata": {
        "id": "H5Wow0wH42f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a TfidfTransformer object is initialized. Don’t worry about the norm=None keyword argument for now, we will dig into this in the next exercise\n",
        "the TfidfTransformer is fit (trained) on a term-document matrix of term frequencies\n",
        "the .idf_ attribute of the TfidfTransformer stores the inverse document frequencies of the terms as a NumPy array"
      ],
      "metadata": {
        "id": "RCqCSJba45LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# term_frequency\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from preprocessing import preprocess_text\n",
        "from poems import poems\n",
        "\n",
        "# preprocess text\n",
        "processed_poems = [preprocess_text(poem) for poem in poems]\n",
        "\n",
        "# initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "term_frequencies = vectorizer.fit_transform(processed_poems)\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# get corpus index\n",
        "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
        "\n",
        "# create pandas DataFrame with term frequencies\n",
        "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)"
      ],
      "metadata": {
        "id": "uBm8RoHj4_8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
        "\n",
        "# display term-document matrix of term frequencies\n",
        "print(df_term_frequencies)\n",
        "\n",
        "# initialize and fit TfidfTransformer\n",
        "transformer = TfidfTransformer(norm=None)\n",
        "transformer.fit(term_frequencies)\n",
        "idf_values = transformer.idf_\n",
        "\n",
        "# create pandas DataFrame with inverse document frequencies\n",
        "try:\n",
        "  df_idf = pd.DataFrame(idf_values, index = feature_names, columns=['Inverse Document Frequency'])\n",
        "  print(df_idf)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "1kve01sX5cIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily calculate the tf-idf values for each term-document pair in our corpus using scikit-learn’s TfidfVectorizer:"
      ],
      "metadata": {
        "id": "VCLdUbTO76Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "tfidf_vectorizer = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "TcZXbcSJ7Rul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a TfidfVectorizer object is initialized. The norm=None keyword argument prevents scikit-learn from modifying the multiplication of term frequency and inverse document frequency\n",
        "the TfidfVectorizer object is fit and transformed on the corpus of data, returning the tf-idf scores for each term-document pair"
      ],
      "metadata": {
        "id": "BhoOponV8Cdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Bag-of-Words to Tf-idf\n",
        "In addition to directly calculating the tf-idf scores for a set of terms across a corpus, you can also convert a bag-of-words model you have already created into tf-idf scores. Scikit-learn’s TfidfTransformer is up to the task of converting your bag-of-words model to tf-idf. You begin by initializing a TfidfTransformer object."
      ],
      "metadata": {
        "id": "WIK_Z8ysDu3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_transformer = TfidfTransformer(norm=False)"
      ],
      "metadata": {
        "id": "bNMMDUGtD3h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a bag-of-words matrix count_matrix, you can now multiply the term frequencies by their inverse document frequency to get the tf-idf scores as follows:"
      ],
      "metadata": {
        "id": "XtrQ8YipEDPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_scores = tfidf_transformer.fit_transform(count_matrix)"
      ],
      "metadata": {
        "id": "F1jUOlHL7-Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is very similar to how we calculated inverse document frequency, except this time we are fitting and transforming the TfidfTransformer to the term frequencies/bag-of-words vectors rather than just fitting the TfidfTransformer to them."
      ],
      "metadata": {
        "id": "8N4rtmKBEQVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from term_frequency import bow_matrix, feature_names, df_bag_of_words, corpus_index\n",
        "\n",
        "# display term-document matrix of term frequencies (bag-of-words)\n",
        "print(df_bag_of_words)\n",
        "\n",
        "# initialize and fit TfidfTransformer, transform bag-of-words matrix\n",
        "transformer = TfidfTransformer(norm=False)\n",
        "tfidf_scores = transformer.fit_transform(bow_matrix)\n",
        "\n",
        "# create pandas DataFrame with tf-idf scores\n",
        "try:\n",
        "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index = feature_names, columns=corpus_index)\n",
        "  print(df_tf_idf)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "q3fIlkHaEUDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_raven_stanzas = the_raven.split('.')"
      ],
      "metadata": {
        "id": "e42qPDmHWzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from raven import the_raven_stanzas\n",
        "from preprocessing import preprocess_text\n",
        "\n",
        "# view first stanza\n",
        "print(the_raven_stanzas[0])\n",
        "\n",
        "# preprocess documents\n",
        "processed_stanzas = [preprocess_text(stanza) for stanza in the_raven_stanzas]\n",
        "\n",
        "# initialize and fit TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "tfidf_scores = vectorizer.fit_transform(processed_stanzas)\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# get stanza index\n",
        "stanza_index = [f\"Stanza {i+1}\" for i in range(len(the_raven_stanzas))]\n",
        "\n",
        "# create pandas DataFrame with tf-idf scores\n",
        "try:\n",
        "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=stanza_index)\n",
        "  print(df_tf_idf)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "zz3_iPUHW1In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with Text Data | scikit-learn | From Occurrences to Frequencies\n",
        "Working with Text Data | scikit-learn | From Occurrences to Frequencies\n",
        "In this documentation, you will learn how to use scikit-learn to conduct tf-idf. This is helpful if you are trying to determine topics or themes within text data.\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies"
      ],
      "metadata": {
        "id": "ppJCxnOHYv-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecademylib3_seaborn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from articles import articles\n",
        "from preprocessing import preprocess_text\n",
        "# import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
        "\n",
        "# view article\n",
        "# print(articles[1])\n",
        "\n",
        "# preprocess articles\n",
        "processed_articles = []\n",
        "for article in articles:\n",
        "  processed_articles.append(preprocess_text(article))\n",
        "# print(processed_articles)\n",
        "# or it can be done like this\n",
        "# processed_articles = [preprocess_text(article) for article in articles]\n",
        "\n",
        "# initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "counts = vectorizer.fit_transform(processed_articles)\n",
        "# convert counts to tf-idf\n",
        "transformer = TfidfTransformer(norm=None)\n",
        "\n",
        "# initialize and fit TfidfVectorizer\n",
        "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
        "# check if tf-idf scores are equal\n",
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
        "\n",
        "# Let’s confirm that the tf-idf scores given by TfidfTransformer and TfidfVectorizer are the same.\n",
        "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
        "  print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
        "else:\n",
        "  print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
        "\n",
        "\n",
        "\n",
        "# get vocabulary of terms\n",
        "try:\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# get article index\n",
        "try:\n",
        "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# create pandas DataFrame with word counts\n",
        "try:\n",
        "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
        "  print(df_word_counts)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# create pandas DataFrame(s) with tf-idf scores\n",
        "try:\n",
        "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
        "  print(df_tf_idf)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
        "  print(df_tf_idf)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# get highest scoring tf-idf term for each article\n",
        "# The Pandas Series method .idxmax() is a helpful tool for returning the index of the highest value in a DataFrame column.\n",
        "for i in range(1,11):\n",
        "  print(df_tf_idf[[f'Article {i}']].idxmax())\n"
      ],
      "metadata": {
        "id": "GhpPCLzO_O78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD EMBEDDINGS"
      ],
      "metadata": {
        "id": "0Cs-kcb5Yyaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# load model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# define vectors\n",
        "summer_vec = nlp(\"summer\").vector\n",
        "winter_vec = nlp(\"winter\").vector\n",
        "\n",
        "# compare similarity\n",
        "print(f\"The cosine distance between the word embeddings for 'summer' and 'winter' is: {cosine(summer_vec, winter_vec)}\\n\")\n",
        "\n",
        "# define vectors\n",
        "mustard_vec = nlp(\"mustard\").vector\n",
        "amazing_vec = nlp(\"amazing\").vector\n",
        "\n",
        "# compare similarity\n",
        "print(f\"The cosine distance between the word embeddings for 'mustard' and 'amazing' is: {cosine(mustard_vec, amazing_vec)}\\n\")\n",
        "\n",
        "# display word embeddings\n",
        "print(f\"'summer' in vector form: {summer_vec}\")\n",
        "print(f\"'winter' in vector form: {winter_vec}\")\n",
        "# print(f\"'mustard' in vector form: {mustard_vec}\")\n",
        "# print(f\"'amazing' in vector form: {amazing_vec}\")"
      ],
      "metadata": {
        "id": "yGiYV5sb-mLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily represent vectors in Python using NumPy arrays. To create a vector containing the odd numbers from 1 to 9, we can use NumPy’s .array() method:\n",
        "\n",
        "odd_vector = np.array([1, 3, 5, 7, 9])"
      ],
      "metadata": {
        "id": "li-HDeIr-pON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import codecademylib3_seaborn\n",
        "\n",
        "# define score vectors\n",
        "scores_xavier = np.array([88, 92])\n",
        "scores_niko = np.array([94, 87])\n",
        "scores_alena = np.array([90, 48])\n",
        "\n",
        "# plot vectors\n",
        "try:\n",
        "  plt.arrow(0, 0, scores_xavier[0], scores_xavier[1], width=1, color='blue')\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  plt.arrow(0, 0, scores_niko[0], scores_niko[1], width=1, color='orange')\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  plt.arrow(0, 0, scores_alena[0], scores_alena[1], width=1, color='purple')\n",
        "except:\n",
        "  pass\n",
        "plt.axis([0, 100, 0, 100])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IN6lMF2V-sKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings are vector representations of a word.\n",
        "\n",
        "They allow us to take all the information that is stored in a word, like its meaning and its part of speech, and convert it into a numeric form that is more understandable to a computer.\n",
        "\n",
        "We can load a basic English word embedding model using spaCy as follows:\n",
        "\n",
        "nlp = spacy.load('en') Note: the convention is to load spaCy models into a variable named nlp.\n",
        "\n",
        "To get the vector representation of a word, we call the model with the desired word as an argument and can use the .vector attribute.\n",
        "\n",
        "nlp('love').vector"
      ],
      "metadata": {
        "id": "wtRs3ekj-v8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# load word embedding model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# define word embedding vectors\n",
        "happy_vec = nlp('happy').vector\n",
        "sad_vec = nlp('sad').vector\n",
        "angry_vec = nlp('angry').vector\n",
        "\n",
        "# find vector length here\n",
        "vector_length = len(happy_vec)\n",
        "print(vector_length)"
      ],
      "metadata": {
        "id": "RFdwFLYn-ybd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key at the heart of word embeddings is distance. We can easily calculate the Manhattan, Euclidean, and cosine distances between vectors using helper functions from SciPy. When working with vectors that have a large number of dimensions, such as word embeddings, the distances calculated by Manhattan and Euclidean distance can become rather large. Thus, calculations using cosine distance are preferred!\n",
        "\n",
        "In Manhattan distance, also known as city block distance, distance is defined as the sum of the differences across each individual dimension of the vectors.\n",
        "\n",
        "Another common distance metric is called the Euclidean distance, also known as straight line distance. With this distance metric, we take the square root of the sum of the squares of the differences in each dimension.\n",
        "\n",
        "The final distance we will consider is the cosine distance. Cosine distance is concerned with the angle between two vectors, rather than by looking at the distance between the points, or ends, of the vectors. Two vectors that point in the same direction have no angle between them, and have a cosine distance of 0. Two vectors that point in opposite directions, on the other hand, have a cosine distance of 1"
      ],
      "metadata": {
        "id": "J0e-dgHc-2O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
        " \n",
        "vector_a = np.array([1,2,3])\n",
        "vector_b = np.array([2,4,6])\n",
        " \n",
        "# Manhattan distance:\n",
        "manhattan_d = cityblock(vector_a,vector_b) # 6\n",
        " \n",
        "# Euclidean distance:\n",
        "euclidean_d = euclidean(vector_a,vector_b) # 3.74\n",
        " \n",
        "# Cosine distance:\n",
        "cosine_d = cosine(vector_a,vector_b) # 0.0"
      ],
      "metadata": {
        "id": "AITfA9Ts-_Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
        "import spacy\n",
        "\n",
        "# load word embedding model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# define word embedding vectors\n",
        "happy_vec = nlp('happy').vector\n",
        "sad_vec = nlp('sad').vector\n",
        "angry_vec = nlp('angry').vector\n",
        "\n",
        "# calculate Manhattan distance\n",
        "man_happy_sad = cityblock(happy_vec,sad_vec)\n",
        "man_sad_angry = cityblock(sad_vec, angry_vec)\n",
        "print(man_happy_sad)\n",
        "print(man_sad_angry)\n",
        "\n",
        "\n",
        "# calculate Euclidean distance\n",
        "euc_happy_sad = euclidean(happy_vec,sad_vec)\n",
        "euc_sad_angry = euclidean(sad_vec, angry_vec)\n",
        "print(euc_happy_sad)\n",
        "print(euc_sad_angry)\n",
        "\n",
        "# calculate cosine distance\n",
        "cos_happy_sad = cosine(happy_vec,sad_vec)\n",
        "cos_sad_angry = cosine(sad_vec, angry_vec)\n",
        "print(cos_happy_sad)\n",
        "print(cos_sad_angry)"
      ],
      "metadata": {
        "id": "UASpEJ1y5JkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cosine distance values, however, remain low and bounded between 0 and 1, where the Manhattan and Euclidean distances are rather large (and continue to grow as more dimensions are added to a vector)."
      ],
      "metadata": {
        "id": "2xMSG6JI5OFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind word embeddings is a theory known as the distributional hypothesis. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. With word embeddings, we map words that exist with the same context to similar places in our vector space (math-speak for the area in which our vectors exist)."
      ],
      "metadata": {
        "id": "3sJzqJegAIb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "from processing import most_common_words, vector_list\n",
        "\n",
        "# print word and vector representation at index 347\n",
        "# print(most_common_words[347])\n",
        "# print(vector_list[347])\n",
        "\n",
        "# define find_closest_words\n",
        "def find_closest_words(word_list, vector_list, word_to_check):\n",
        "    return sorted(word_list,\n",
        "                  key=lambda x: cosine(vector_list[word_list.index(word_to_check)], vector_list[word_list.index(x)]))[:10]\n",
        "\n",
        "# find closest words to food\n",
        "close_to_food = find_closest_words(most_common_words, vector_list,\"food\")\n",
        "print(close_to_food)\n",
        "\n",
        "# find closest words to summer\n",
        "\n",
        "close_to_summer = find_closest_words(most_common_words, vector_list,\"summer\")\n",
        "print(close_to_summer)"
      ],
      "metadata": {
        "id": "i16kLIN85LXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec\n",
        "\n",
        "Step in word2vec! Word2vec is a statistical learning algorithm that develops word embeddings from a corpus of text. Word2vec uses one of two different model architectures to come up with the values that define a collection of word embeddings."
      ],
      "metadata": {
        "id": "0VPwcUzRBDLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One method is to use the continuous bag-of-words (CBOW) representation of a piece of text. The word2vec model goes through each word in the training corpus, in order, and tries to predict what word comes at each position based on applying bag-of-words to the words that surround the word in question. In this approach, the order of the words does not matter!\n",
        "\n",
        "The other method word2vec can use to create word embeddings is continuous skip-grams. Skip-grams function similarly to n-grams, except instead of looking at groupings of n-consecutive words in a text, we can look at sequences of words that are separated by some specified distance between them.\n",
        "\n",
        "When using continuous skip-grams, the order of context is taken into consideration! Because of this, the time it takes to train the word embeddings is slower than when using continuous bag-of-words. The results, however, are often much better!"
      ],
      "metadata": {
        "id": "20kjzayxCYNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentence = \"It was the best of times, it was the worst of times.\"\n",
        "print(sentence)\n",
        "\n",
        "# preprocessing\n",
        "sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]\n",
        "\n",
        "# set context_length\n",
        "# This indicates that when finding our bag-of-words and skip-gram representations, we are only looking 2 words to the left and 2 words to the right of our word we are focusing on.\n",
        "# to increase number here means that when word2vec trains our model, a larger context will be taken into consideration!\n",
        "context_length = 3\n",
        "\n",
        "# function to get cbows\n",
        "def get_cbows(sentence_lst, context_length):\n",
        "  cbows = list()\n",
        "  for i, val in enumerate(sentence_lst):\n",
        "    if i < context_length:\n",
        "      pass\n",
        "    elif i < len(sentence_lst) - context_length:\n",
        "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
        "      vectorizer = CountVectorizer()\n",
        "      vectorizer.fit_transform(context)\n",
        "      context_no_order = vectorizer.get_feature_names()\n",
        "      cbows.append((val,context_no_order))\n",
        "  return cbows\n",
        "\n",
        "# define cbows here:\n",
        "cbows = get_cbows(sentence_lst, context_length)\n",
        "\n",
        "\n",
        "# function to get cbows\n",
        "def get_skip_grams(sentence_lst, context_length):\n",
        "  skip_grams = list()\n",
        "  for i, val in enumerate(sentence_lst):\n",
        "    if i < context_length:\n",
        "      pass\n",
        "    elif i < len(sentence_lst) - context_length:\n",
        "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
        "      skip_grams.append((val, context))\n",
        "  return skip_grams\n",
        "\n",
        "# define skip_grams here:\n",
        "skip_grams = get_skip_grams(sentence_lst,context_length)\n",
        "\n",
        "try:\n",
        "  print('\\nContinuous Bag of Words')\n",
        "  for cbow in cbows:\n",
        "    print(cbow)\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  print('\\nSkip Grams')\n",
        "  for skip_gram in skip_grams:\n",
        "    print(skip_gram)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "KWl4TSlKGfyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the words themselves do not vary between the output of get_cbows() and get_skip_grams(), the order is different! With continuous skip-grams we do care about word order, so the order is preserved in our output."
      ],
      "metadata": {
        "id": "GsHs9t47GeW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim\n",
        "\n",
        "When we want to train our own word2vec model on a corpus of text, we can use the gensim package! With gensim, however, we are able to build our own word embeddings on any corpus of text we like.\n",
        "\n",
        "To easily train a word2vec model on our own corpus of text, we can use gensim’s Word2Vec() function."
      ],
      "metadata": {
        "id": "otc5c1t6Jgo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(corpus, size=100, window=5, min_count=1, workers=2, sg=1)"
      ],
      "metadata": {
        "id": "rNg_EkUpK5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus is a list of lists, where each inner list is a document in the corpus and each element in the inner lists is a word token\n",
        "size determines how many dimensions our word embeddings will include. Word embeddings often have upwards of 1,000 dimensions! Here we will create vectors of 100-dimensions to keep things simple."
      ],
      "metadata": {
        "id": "sVVcbyyqK5Jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the entire vocabulary used to train the word embedding model, we can use the .wv.vocab.items() method."
      ],
      "metadata": {
        "id": "WWbxEHI6LoEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_of_model = list(model.wv.vocab.items())"
      ],
      "metadata": {
        "id": "KkGXFPS1Lo7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To easily find which vectors gensim placed close together in its word embedding model, we can use the .most_similar() method."
      ],
      "metadata": {
        "id": "lK9og1f_L-QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"my_word_here\", topn=100)"
      ],
      "metadata": {
        "id": "AyMRy7CXLrfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"my_word_here\" is the target word token we want to find most similar words to\n",
        "topn is a keyword argument that indicates how many similar word vectors we want returned"
      ],
      "metadata": {
        "id": "2GTIwoLhMJg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.doesnt_match([\"asia\", \"mars\", \"pluto\"])"
      ],
      "metadata": {
        "id": "7A38p8-zMKOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "when given a list of terms in the vocabulary as an argument, .doesnt_match() returns which term is furthest from the others."
      ],
      "metadata": {
        "id": "sVXYnDntMPO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from romeo_juliet import romeo_and_juliet\n",
        "\n",
        "# load stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# preprocess text\n",
        "romeo_and_juliet_processed = [[word for word in romeo_and_juliet.lower().split() if word not in stop_words]]\n",
        "\n",
        "# view inner list of romeo_and_juliet_processed\n",
        "print(romeo_and_juliet_processed[0][:20])\n",
        "\n",
        "# train word embeddings model\n",
        "model = gensim.models.Word2Vec(romeo_and_juliet_processed, size=100, window=5, min_count=1, workers=2, sg=1)\n",
        "\n",
        "# view vocabulary\n",
        "vocabulary = list(model.wv.vocab.items())\n",
        "#print(vocabulary)\n",
        "\n",
        "# similar to romeo\n",
        "similar_to_romeo = model.most_similar(\"romeo\", topn=20)\n",
        "print(similar_to_romeo)\n",
        "\n",
        "\n",
        "# one is not like the others\n",
        "not_star_crossed_lover = model.doesnt_match([\"romeo\", \"juliet\", \"mercutio\"])\n",
        "print(not_star_crossed_lover)"
      ],
      "metadata": {
        "id": "RyY3Vrj1GEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "U.S.A. Presidential Vocabulary project"
      ],
      "metadata": {
        "id": "cU7djK1TGDlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper"
      ],
      "metadata": {
        "id": "crSGgo6zvU_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from collections import Counter\n",
        "\n",
        "def read_file(file_name):\n",
        "  with open(file_name, 'r+', encoding='utf-8') as file:\n",
        "    file_text = file.read()\n",
        "  return file_text\n",
        "\n",
        "def process_speeches(speeches):\n",
        "  word_tokenized_speeches = list()\n",
        "  for speech in speeches:\n",
        "    sentence_tokenizer = PunktSentenceTokenizer()\n",
        "    sentence_tokenized_speech = sentence_tokenizer.tokenize(speech)\n",
        "    word_tokenized_sentences = list()\n",
        "    for sentence in sentence_tokenized_speech:\n",
        "      word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
        "      word_tokenized_sentences.append(word_tokenized_sentence)\n",
        "    word_tokenized_speeches.append(word_tokenized_sentences)\n",
        "  return word_tokenized_speeches\n",
        "\n",
        "def merge_speeches(speeches):\n",
        "  all_sentences = list()\n",
        "  for speech in speeches:\n",
        "    for sentence in speech:\n",
        "      all_sentences.append(sentence)\n",
        "  return all_sentences\n",
        "\n",
        "def get_president_sentences(president):\n",
        "  files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
        "  speeches = [read_file(file) for file in files]\n",
        "  processed_speeches = process_speeches(speeches)\n",
        "  all_sentences = merge_speeches(processed_speeches)\n",
        "  return all_sentences\n",
        "\n",
        "def get_presidents_sentences(presidents):\n",
        "  all_sentences = list()\n",
        "  for president in presidents:\n",
        "    files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
        "    speeches = [read_file(file) for file in files]\n",
        "    processed_speeches = process_speeches(speeches)\n",
        "    all_prez_sentences = merge_speeches(processed_speeches)\n",
        "    all_sentences.extend(all_prez_sentences)\n",
        "  return all_sentences\n",
        "\n",
        "def most_frequent_words(list_of_sentences):\n",
        "  all_words = [word for sentence in list_of_sentences for word in sentence]\n",
        "  return Counter(all_words).most_common()"
      ],
      "metadata": {
        "id": "A-jYQZ6vrIVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calling process_speeches() with speeches as an argument, the data will be formatted such that:\n",
        "\n",
        "processed_speeches[0] represents the first inaugural address in processed_speeches.\n",
        "processed_speeches[0][0] represents the first sentence in the first inaugural address in processed_speeches.\n",
        "processed_speeches[0][0][0] represents the first word in the first sentence in the first inaugural address in processed_speeches."
      ],
      "metadata": {
        "id": "vSubunV9v58_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gensim\n",
        "import spacy\n",
        "from president_helper import read_file, process_speeches, merge_speeches, get_president_sentences, get_presidents_sentences, most_frequent_words\n",
        "\n",
        "# get list of all speech files\n",
        "files = sorted([file for file in os.listdir() if file[-4:] == '.txt'])\n",
        "# print(files)\n",
        "\n",
        "# read each speech file\n",
        "speeches = []\n",
        "for file in files:\n",
        "  speeches.append(read_file(file))\n",
        "\n",
        "# preprocess each speech\n",
        "processed_speeches = process_speeches(speeches)\n",
        "# merge speeches\n",
        "all_sentences = merge_speeches(processed_speeches)\n",
        "\n",
        "# view most frequently used words\n",
        "most_freq_words = most_frequent_words(all_sentences)\n",
        "# print(most_freq_words)\n",
        "\n",
        "# create gensim model of all speeches\n",
        "all_prez_embeddings = gensim.models.Word2Vec(all_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
        "\n",
        "# view words similar to freedom\n",
        "similar_to_freedom = all_prez_embeddings.most_similar(\"freedom\", topn = 20)\n",
        "# print(similar_to_freedom)\n",
        "\n",
        "similar_to_problems = all_prez_embeddings.most_similar(\"problems\", topn = 20)\n",
        "# print(similar_to_problems)\n",
        "\n",
        "# get President Roosevelt sentences\n",
        "roosevelt_sentences = get_president_sentences(\"franklin-d-roosevelt\")\n",
        "\n",
        "# view most frequently used words of Roosevelt\n",
        "roosevelt_most_freq_words = most_frequent_words(roosevelt_sentences)\n",
        "# print(roosevelt_most_freq_words)\n",
        "\n",
        "# create gensim model for Roosevelt\n",
        "roosevelt_embeddings = gensim.models.Word2Vec(roosevelt_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
        "\n",
        "# view words similar to freedom for Roosevelt\n",
        "roosevelt_similar_to_freedom = roosevelt_embeddings.most_similar(\"problems\", topn = 20)\n",
        "# print(roosevelt_similar_to_freedom)\n",
        "\n",
        "# get sentences of multiple presidents\n",
        "rushmore_prez_sentences = get_presidents_sentences([\"washington\",\"jefferson\",\"lincoln\",\"theodore-roosevelt\"])\n",
        "\n",
        "# view most frequently used words of presidents\n",
        "rushmore_most_freq_words = most_frequent_words(rushmore_prez_sentences) \n",
        "#print(rushmore_most_freq_words)\n",
        "\n",
        "# create gensim model for the presidents\n",
        "rushmore_embeddings =  gensim.models.Word2Vec(rushmore_prez_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
        "\n",
        "# view words similar to freedom for presidents\n",
        "rushmore_similar_to_freedom = rushmore_embeddings.most_similar(\"freedom\", topn = 20)\n",
        "print(rushmore_similar_to_freedom)\n",
        "\n",
        "rushmore_similar_to_problems = rushmore_embeddings.most_similar(\"problems\", topn = 20)\n",
        "print(rushmore_similar_to_problems)"
      ],
      "metadata": {
        "id": "2DhtBfDgv6zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING TEXT WITH DEEP LEARNING"
      ],
      "metadata": {
        "id": "PsHqm6Kw6y1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for seq2seq"
      ],
      "metadata": {
        "id": "gtupnCtl62dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are a few neural network libraries, we’ll be using TensorFlow with the Keras API to build a pretty limited English-to-Spanish translator "
      ],
      "metadata": {
        "id": "wAr0nb4x7E8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "ZdtknpKk61dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll need the following for our Keras implementation:\n",
        "\n",
        "vocabulary sets for both our input (English) and target (Spanish) data\n",
        "the total number of unique word tokens we have for each set\n",
        "the maximum sentence length we’re using for each language"
      ],
      "metadata": {
        "id": "kMasE3GR7u5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import re\n",
        "# Importing our translations\n",
        "data_path = \"span-eng.txt\"\n",
        "# Defining lines as a list of each line\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "# Building empty lists to hold sentences\n",
        "input_docs = []\n",
        "target_docs = []\n",
        "# Building empty vocabulary sets\n",
        "input_tokens = set()\n",
        "target_tokens = set()\n",
        "\n",
        "for line in lines:\n",
        "  # Input and target sentences are separated by tabs\n",
        "  input_doc, target_doc = line.split('\\t')\n",
        "  # Appending each input sentence to input_docs\n",
        "  input_docs.append(input_doc)\n",
        "  # Splitting words from punctuation\n",
        "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
        "  # Redefine target_doc below \n",
        "  target_doc = \"<START> \" + target_doc + \" <END>\"\n",
        "  # and append it to target_docs:\n",
        "  target_docs.append(target_doc)\n",
        "  \n",
        "  # Now we split up each sentence into words\n",
        "  # and add each unique word to our vocabulary set\n",
        "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
        "    print(token)\n",
        "    # Add your code here:\n",
        "    if token not in input_tokens:\n",
        "      input_tokens.add(token)\n",
        "    \n",
        "  for token in target_doc.split():\n",
        "    print(token)\n",
        "    # And here:\n",
        "    if token not in target_tokens:\n",
        "      target_tokens.add(token)\n",
        "    \n",
        "\n",
        "input_tokens = sorted(list(input_tokens))\n",
        "target_tokens = sorted(list(target_tokens))\n",
        "\n",
        "# Create num_encoder_tokens and num_decoder_tokens:\n",
        "num_encoder_tokens = len(input_tokens)\n",
        "num_decoder_tokens = len(target_tokens)\n",
        "\n",
        "try:\n",
        "  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
        "  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
        "except ValueError:\n",
        "  pass"
      ],
      "metadata": {
        "id": "LH9jmYSy30CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup (part 1)\n",
        " In a one-hot vector, every token in our set is represented by a 0 except for the current token which is represented by a 1. For example given the vocabulary [\"the\", \"dog\", \"licked\", \"me\"], a one-hot vector for “dog” would look like [0, 1, 0, 0].\n",
        " Because each matrix is almost all zeros, we’ll use numpy.zeros() from the NumPy library to build them out."
      ],
      "metadata": {
        "id": "lK1TzQcG307H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')"
      ],
      "metadata": {
        "id": "FJtD9Aj6xjsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the shape of the matrix — in our case the number of documents (or sentences) by the maximum token sequence length (the longest sentence we want to see) by the number of unique tokens (or words)\n",
        "the data type we want — in our case NumPy’s float32, which can speed up our processing a bit"
      ],
      "metadata": {
        "id": "Oe1MJrABySL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import re\n",
        "# Importing our translations\n",
        "data_path = \"span-eng.txt\"\n",
        "# Defining lines as a list of each line\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "# Building empty lists to hold sentences\n",
        "input_docs = []\n",
        "target_docs = []\n",
        "# Building empty vocabulary sets\n",
        "input_tokens = set()\n",
        "target_tokens = set()\n",
        "\n",
        "for line in lines:\n",
        "  # Input and target sentences are separated by tabs\n",
        "  input_doc, target_doc = line.split('\\t')\n",
        "  # Appending each input sentence to input_docs\n",
        "  input_docs.append(input_doc)\n",
        "  # Splitting words from punctuation\n",
        "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
        "  # Redefine target_doc below \n",
        "  # and append it to target_docs:\n",
        "  target_doc = '<START> ' + target_doc + ' <END>'\n",
        "  target_docs.append(target_doc)\n",
        "  \n",
        "  # Now we split up each sentence into words\n",
        "  # and add each unique word to our vocabulary set\n",
        "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
        "    print(token)\n",
        "    # Add your code here:\n",
        "    if token not in input_tokens:\n",
        "      input_tokens.add(token)\n",
        "  for token in target_doc.split():\n",
        "    print(token)\n",
        "    # And here:\n",
        "    if token not in target_tokens:\n",
        "      target_tokens.add(token)\n",
        "\n",
        "input_tokens = sorted(list(input_tokens))\n",
        "target_tokens = sorted(list(target_tokens))\n",
        "\n",
        "# Create num_encoder_tokens and num_decoder_tokens:\n",
        "num_encoder_tokens = len(input_tokens)\n",
        "num_decoder_tokens = len(target_tokens)\n",
        "\n",
        "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
        "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n"
      ],
      "metadata": {
        "id": "RZi3J-m-yRXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n",
        "print('Number of samples:', len(input_docs))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "input_features_dict = dict(\n",
        "    [(token, i) for i, token in enumerate(input_tokens)])\n",
        "# Build out target_features_dict:\n",
        "target_features_dict =  dict([(token, i) for i, token in enumerate(target_tokens)])\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_features_dict = dict(\n",
        "    (i, token) for token, i in input_features_dict.items())\n",
        "# Build out reverse_target_features_dict:\n",
        "reverse_target_features_dict = dict(\n",
        "    (i, token) for token, i in target_features_dict.items())\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "print(\"\\nHere's the first item in the encoder input matrix:\\n\", encoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")\n",
        "\n",
        "# Build out the decoder_input_data matrix:\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "# Build out the decoder_target_data matrix:\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "metadata": {
        "id": "rFbRvW374Km3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup (part 2)\n",
        "To build out a three-dimensional NumPy matrix of one-hot vectors, we can assign a value of 1 for a given word at a given timestep in a given line:"
      ],
      "metadata": {
        "id": "TCpHiEzK1HJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_name[line, timestep, features_dict[token]] = 1."
      ],
      "metadata": {
        "id": "RFyrqapj1Hzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras will fit — or train — the seq2seq model using these matrices of one-hot vectors:\n",
        "\n",
        "the encoder input data\n",
        "the decoder input data\n",
        "the decoder target data"
      ],
      "metadata": {
        "id": "eZOXwEKF3bRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import re\n",
        "from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n",
        "input_features_dict = dict(\n",
        "    [(token, i) for i, token in enumerate(input_tokens)])\n",
        "target_features_dict = dict(\n",
        "    [(token, i) for i, token in enumerate(target_tokens)])\n",
        "\n",
        "reverse_input_features_dict = dict(\n",
        "    (i, token) for token, i in input_features_dict.items())\n",
        "reverse_target_features_dict = dict(\n",
        "    (i, token) for token, i in target_features_dict.items())\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
        "\n",
        "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
        "\n",
        "    print(\"Encoder input timestep & token:\", timestep, token)\n",
        "    print(input_features_dict[token])\n",
        "    # Assign 1. for the current line, timestep, & word\n",
        "    # in encoder_input_data:\n",
        "    encoder_input_data[line, timestep, input_features_dict[token]] = 1\n",
        "\n",
        "  for timestep, token in enumerate(target_doc.split()):\n",
        "\n",
        "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "    print(\"Decoder input timestep & token:\", timestep, token)\n",
        "    # Assign 1. for the current line, timestep, & word\n",
        "    # in decoder_input_data:\n",
        "    decoder_input_data[line, timestep, target_features_dict[token]] = 1\n",
        "    if timestep > 0:\n",
        "      # decoder_target_data is ahead by 1 timestep\n",
        "      # and doesn't include the start token.\n",
        "      print(\"Decoder target timestep:\", timestep)\n",
        "      # Assign 1. for the current line, timestep, & word\n",
        "      # in decoder_target_data:\n",
        "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1"
      ],
      "metadata": {
        "id": "C1Nk1WjJ3cGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Training Setup\n",
        "\n",
        "Our encoder requires two layer types from Keras:\n",
        "\n",
        "An input layer, which defines a matrix to hold all the one-hot vectors that we’ll feed to the model.\n",
        "An LSTM layer, with some output dimensionality.\n",
        "We can import these layers as well as the model we need like so:"
      ],
      "metadata": {
        "id": "Cz1eTNz9qzdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "efmJkWyt7mBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up the input layer, which requires some number of dimensions that we’re providing. In this case, we know that we’re passing in all the encoder tokens, but we don’t necessarily know our batch size. Fortunately, we can say None because the code is written to handle varying batch sizes, so we don’t need to specify that dimension."
      ],
      "metadata": {
        "id": "l_Pns7anrJZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the shape specifies the input matrix sizes\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))"
      ],
      "metadata": {
        "id": "VV3hiIeorUgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the LSTM layer, we need to select the dimensionality (the size of the LSTM’s hidden states, which helps determine how closely the model molds itself to the training data — something we can play around with) and whether to return the state (in this case we do):"
      ],
      "metadata": {
        "id": "g07BGCi5ri5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_lstm = LSTM(100, return_state=True)\n",
        "# we're using a dimensionality of 100\n",
        "# so any LSTM output matrix will have \n",
        "# shape [batch_size, 100]"
      ],
      "metadata": {
        "id": "supM7XKMrj4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, the only thing we want from the encoder is its final states. We can get these by linking our LSTM layer with our input layer:"
      ],
      "metadata": {
        "id": "ferdsDi_rqKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)"
      ],
      "metadata": {
        "id": "iTJSfdUjrs3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder_outputs isn’t really important for us, so we can just discard it. However, the states, we’ll save in a list:"
      ],
      "metadata": {
        "id": "hAXpj6d_rwjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_states = [state_hidden, state_cell]"
      ],
      "metadata": {
        "id": "PsVZYv4Frv6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We’ve moved the code from the previous exercises into another file\n",
        "\n",
        "from prep import num_encoder_tokens\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "# Create the input layer:\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "# Create the LSTM layer:\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "\n",
        "# Retrieve the outputs and states:\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "# Put the states together in a list:\n",
        "encoder_states = [state_hidden, state_cell]"
      ],
      "metadata": {
        "id": "MAgrZ4rCr7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder looks a lot like the encoder (phew!), with an input layer and an LSTM layer that we use together:"
      ],
      "metadata": {
        "id": "yB4MdwcDstar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(100, return_sequences=True, return_state=True)\n",
        "# This time we care about full return sequences"
      ],
      "metadata": {
        "id": "ThaTerwhszDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, with our decoder, we pass in the state data from the encoder, along with the decoder inputs. This time, we’ll keep the output instead of the states:"
      ],
      "metadata": {
        "id": "6fD4ZN66s2za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The two states will be discarded for now\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = \n",
        "    decoder_lstm(decoder_inputs, initial_state=encoder_states)"
      ],
      "metadata": {
        "id": "MXAaYWUfs3X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to run the output through a final activation layer, using the Softmax function, that will give us the probability distribution — where all probabilities sum to one — for each token. The final layer also transforms our LSTM output from a dimensionality of whatever we gave it (in our case, 10) to the number of unique words within the hidden layer’s vocabulary (i.e., the number of unique target tokens, which is definitely more than 10!)."
      ],
      "metadata": {
        "id": "xtuDlTKls6op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        " \n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "YprqOBhDs8UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras’s implementation could work with several layer types, but Dense is the least complex, so we’ll go with that. We also need to modify our import statement to include it before running the code:"
      ],
      "metadata": {
        "id": "OBn2xs-8s_YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense"
      ],
      "metadata": {
        "id": "29_-t5oItBUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we’ve already set up the decoder input and LSTM layers\n",
        "from prep import num_encoder_tokens, num_decoder_tokens\n",
        "\n",
        "from tensorflow import keras\n",
        "# Add Dense to the imported layers\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# Encoder training setup\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "# The decoder input and LSTM layers:\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "# Retrieve the LSTM outputs and states:\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "# Build a final Dense layer:\n",
        "decoder_dense =  Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "# Filter outputs through the Dense layer:\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "v_9r0wAytCh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and Train seq2seq\n",
        "\n",
        "First, we define the seq2seq model using the Model() function we imported from Keras. To make it a seq2seq model, we feed it the encoder and decoder inputs, as well as the decoder output:"
      ],
      "metadata": {
        "id": "qGozIEbPuQnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "Sy-YDHQNuXlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, our model is ready to train. First, we compile everything. Keras models demand two arguments to compile:\n",
        "\n",
        "An optimizer (we’re using RMSprop, which is a fancy version of the widely-used gradient descent) to help minimize our error rate (how bad the model is at guessing the true next word given the previous words in a sentence).\n",
        "A loss function (we’re using the logarithm-based cross-entropy function) to determine the error rate.\n",
        "Because we care about accuracy, we’re adding that into the metrics to pay attention to while training. Here’s what the compiling code looks like:"
      ],
      "metadata": {
        "id": "WANHm8IyuW4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TO1LK2jZudcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to fit the compiled model. To do this, we give the .fit() method the encoder and decoder input data (what we pass into the model), the decoder target data (what we expect the model to return given the data we passed in), and some numbers we can adjust as needed:\n",
        "\n",
        "batch size (smaller batch sizes mean more time, and for some problems, smaller batch sizes will be better, while for other problems, larger batch sizes are better)\n",
        "the number of epochs or cycles of training (more epochs mean a model that is more trained on the dataset, and that the process will take more time)\n",
        "validation split (what percentage of the data should be set aside for validating — and determining when to stop training your model — rather than training)\n",
        "Keras will take it from here to get you a (hopefully) nicely trained seq2seq model:"
      ],
      "metadata": {
        "id": "Z5HjbGxvuifZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], \n",
        "          decoder_target_data,\n",
        "          batch_size=10,\n",
        "          epochs=100,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "PtjONw9mujK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prep import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "from tensorflow import keras\n",
        "# Add Dense to the imported layers\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# Encoder training setup\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "# Decoder training setup:\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Building the training model:\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "print(\"Model summary:\\n\")\n",
        "training_model.summary()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Compile the model:\n",
        "training_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Choose the batch size \n",
        "# and number of epochs:\n",
        "# Because we don’t want to crash this exercise, we’ll make the batch size large and the number of epochs very small. (Note that small batch sizes are more prone to crashing a deep learning program in general, but in our case we care about time.)\n",
        "batch_size = 50\n",
        "epochs = 50\n",
        "\n",
        "print(\"Training the model:\\n\")\n",
        "# Train the model:\n",
        "training_model.fit([encoder_input_data, decoder_input_data],decoder_target_data, batch_size=batch_size, epochs=epochs,validation_split=0.2)"
      ],
      "metadata": {
        "id": "fhgV9hegvwjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for Testing\n",
        "\n",
        "However, to generate some original output text, we need to redefine the seq2seq architecture in pieces. The model we used for training our network only works when we already know the target sequence. This time, we have no idea what the Spanish should be for the English we pass in! So we need a model that will decode step-by-step instead of using teacher forcing. To do this, we need a seq2seq network in individual pieces."
      ],
      "metadata": {
        "id": "V4xN6y0avxS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we’ll build an encoder model with our encoder inputs and the placeholders for the encoder’s output states:\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "Next up, we need placeholders for the decoder’s input states, which we can build as input layers and store together. Why? We don’t know what we want to decode yet or what hidden state we’re going to end up with, so we need to do everything step-by-step. We need to pass the encoder’s final hidden state to the decoder, sample a token, and get the updated hidden state back. Then we’ll be able to (manually) pass the updated hidden state back into the network:\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        " \n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        " \n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "Using the decoder LSTM and decoder dense layer (with the activation function) that we trained earlier, we’ll create new decoder states and outputs:\n",
        "\n",
        "decoder_outputs, state_hidden, state_cell = \n",
        "    decoder_lstm(decoder_inputs, \n",
        "    initial_state=decoder_states_inputs)\n",
        " \n",
        "# Saving the new LSTM output states:\n",
        "decoder_states = [state_hidden, state_cell]\n",
        " \n",
        "# Below, we redefine the decoder output\n",
        "# by passing it through the dense layer:\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "Finally, we can set up the decoder model. This is where we bring together:\n",
        "\n",
        "the decoder inputs (the decoder input layer)\n",
        "the decoder input states (the final states from the encoder)\n",
        "the decoder outputs (the NumPy matrix we get from the final output layer of the decoder)\n",
        "the decoder output states (the memory throughout the network from one word to the next)\n",
        "decoder_model = Model(\n",
        "  [decoder_inputs] + decoder_states_inputs,\n",
        "  [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "AzmFSzvuwOnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "training_model = load_model('training_model.h5')\n",
        "# These next lines are only necessary\n",
        "# because we're using a saved model:\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "\n",
        "# Building the encoder test model:\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "# Building the two decoder state input layers:\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "\n",
        "# Put the state input layers into a list:\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "\n",
        "# Call the decoder LSTM:\n",
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs,initial_state=decoder_states_inputs)\n",
        " # Saving the new LSTM output states:\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "\n",
        "# Redefine the decoder outputs:\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Build the decoder test model:\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "1anJ8HrpwPmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Test Function\n",
        "\n",
        "The Test Function\n",
        "Finally, we can get to testing our model! To do this, we need to build a function that:\n",
        "\n",
        "accepts a NumPy matrix representing the test English sentence input\n",
        "uses the encoder and decoder we’ve created to generate Spanish output\n",
        "Inside the test function, we’ll run our new English sentence through the encoder model. The .predict() method takes in new input (as a NumPy matrix) and gives us output states that we can pass on to the decoder:\n",
        "\n",
        "# test_input is a NumPy matrix\n",
        "# representing an English sentence\n",
        "states = encoder.predict(test_input)\n",
        "Next, we’ll build an empty NumPy array for our Spanish translation, giving it three dimensions:\n",
        "\n",
        "# batch size: 1\n",
        "# number of tokens to start with: 1\n",
        "# number of tokens in our target vocabulary\n",
        "target_sequence = np.zeros((1, 1, num_decoder_tokens))\n",
        "Luckily, we already know the first value in our Spanish sentence — \"<Start>\"! So we can give \"<Start>\" a value of 1 at the first timestep:\n",
        "\n",
        "target_sequence[0, 0, target_features_dict['<START>']] = 1.\n",
        "Before we get decoding, we’ll need a string where we can add our translation to, word by word:\n",
        "\n",
        "decoded_sentence = ''\n",
        "This is the variable that we will ultimately return from the function."
      ],
      "metadata": {
        "id": "Xl-U533axs24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n",
        "\n",
        "from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "import numpy as np\n",
        "\n",
        "training_model = load_model('training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence(test_input):\n",
        "  # Encode the input as state vectors:\n",
        "  encoder_states_value = encoder_model.predict(test_input)\n",
        "  # Set decoder states equal to encoder final states\n",
        "  decoder_states_value = encoder_states_value\n",
        "\n",
        "  # Generate empty target sequence of length 1:\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  \n",
        "  # Populate the first token of target sequence with the start token:\n",
        "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "  \n",
        "  decoded_sentence = ''\n",
        "\n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(10):\n",
        "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(test_input)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_docs[seq_index])\n",
        "  print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "id": "ZEyWUls5yAE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Function (part 2)\n",
        "At long last, it’s translation time. Inside the test function, we’ll decode the sentence word by word using the output state that we retrieved from the encoder (which becomes our decoder’s initial hidden state). We’ll also update the decoder hidden state after each word so that we use previously decoded words to help decode new ones.\n",
        "\n",
        "To tackle one word at a time, we need a while loop that will run until one of two things happens (we don’t want the model generating words forever):\n",
        "\n",
        "The current token is \"<END>\".\n",
        "The decoded Spanish sentence length hits the maximum target sentence length.\n",
        "Inside the while loop, the decoder model can use the current target sequence (beginning with the \"<START>\" token) and the current state (initially passed to us from the encoder model) to get a bunch of possible next words and their corresponding probabilities. In Keras, it looks something like this:\n",
        "\n",
        "output_tokens, new_decoder_hidden_state, new_decoder_cell_state = \n",
        "    decoder_model.predict(\n",
        "    [target_seq] + decoder_states_value)\n",
        "Next, we can use NumPy’s .argmax() method to determine the token (word) with the highest probability and add it to the decoded sentence:\n",
        "\n",
        "# slicing [0, -1, :] gives us a\n",
        "# specific token vector within the\n",
        "# 3d NumPy matrix\n",
        "sampled_token_index = np.argmax(\n",
        "    output_tokens[0, -1, :])\n",
        " \n",
        "# The reverse features dictionary\n",
        "# translates back from index to Spanish\n",
        "sampled_token = reverse_target_features_dict[\n",
        "    sampled_token_index]\n",
        " \n",
        "decoded_sentence += \" \" + sampled_token\n",
        "Our final step is to update a few values for the next word in the sequence:\n",
        "\n",
        "# Move to the next timestep \n",
        "# of the target sequence:\n",
        "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "target_seq[0, 0, sampled_token_index] = 1.\n",
        " \n",
        "# Update the states with values from\n",
        "# the most recent decoder prediction:\n",
        "decoder_states_value = [\n",
        "    new_decoder_hidden_state,\n",
        "    new_decoder_cell_state]\n",
        "And now we can test it all out!\n",
        "\n",
        "You may recall that, because of platform constraints here, we’re using very little data. As a result, we can only expect our model to translate a handful of sentences coherently. Luckily, you will have an opportunity to try this out on your own computer with far more data to see some much more impressive results."
      ],
      "metadata": {
        "id": "FJQepmzRA4qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n",
        "\n",
        "from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "import numpy as np\n",
        "\n",
        "training_model = load_model('training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence(test_input):\n",
        "  encoder_states_value = encoder_model.predict(test_input)\n",
        "  decoder_states_value = encoder_states_value\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "  decoded_sentence = ''\n",
        "  \n",
        "  stop_condition = False\n",
        "  while not stop_condition:\n",
        "    # Run the decoder model to get possible \n",
        "    # output tokens (with probabilities) & states\n",
        "    output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)\n",
        "\n",
        "    # Choose token with highest probability\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop token.\n",
        "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    # Update states\n",
        "    decoder_states_value = [\n",
        "    new_decoder_hidden_state,\n",
        "    new_decoder_cell_state]\n",
        "\n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(10):\n",
        "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(test_input)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_docs[seq_index])\n",
        "  print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "id": "gU_tp3chA4JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coursera"
      ],
      "metadata": {
        "id": "GliukvMd4fHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = None\n",
        "    h = 1/(1 + np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return h"
      ],
      "metadata": {
        "id": "mECoke3a4gjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = len(x)\n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x, theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = 1/(1 + np.exp(-z))\n",
        "\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = -1/m*(np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))       \n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha / m) * np.dot(x.T, (h - y))\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "Roq93st84kTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}